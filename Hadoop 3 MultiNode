# Connect To the DataCenter with Public Key or Private in case of windows through Putty or Powershell.

ssh -i Downloads/file.pem ubuntu@public_dns_address

# Update the system
sudo apt-get update && sudo apt-get dist-upgrade -y

# Copy public key on to the DataCenter main server (open a new terminal of powershell copy below command)
scp -i ak1.pem ak1.pem ubuntu@public_dns:~/.ssh (use public ip of connected instance after that ip put :~/.ssh)

# Create a Hadoop user for accessing HDFS
sudo addgroup hadoop
sudo adduser hduser --ingroup hadoop 
sudo adduser hduser sudo
sudo su hduser

# Create local key
ssh-keygen
cat id_ed25519.pub >> authorized_keys
OR
cat id_rsa.pub >> authorized_keys(do cd .ssh and then fired this command)
((after this do cd and then ssh localhost))

# Copy the instance public key (multi.pem) to hduser's directory
sudo su
cp /home/ubuntu/.ssh/multi.pem /home/hduser/.ssh/
chown hduser:hadoop /home/hduser/.ssh/multi.pem
exit
(after doing above steps do cd from ssh to hduser)

# Install Java 8 (Open-JDK)
sudo apt install openjdk-8-jdk openjdk-8-jre -y
java -version


# Download and Install Hadoop

wget https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.1.tar.gz (download from hadoop.apache.org-downloads-miror site-stable-copy link)
tar -xzvf hadoop-3.4.1.tar.gz
sudo mv hadoop-3.4.1 /usr/local/hadoop
sudo chown -R hduser:hadoop /usr/local/hadoop

# Set Enviornment Variable
readlink -f $(which java)
nano ~/.bashrc

export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
export PATH=$PATH:/usr/local/hadoop/bin/
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop

source ~/.bashrc
((after above comnd do below step to setup the hadoop envirnmnet in given location))

cd /usr/local/hadoop/etc/hadoop/

#Update hadoop-env.sh
nano hadoop-env.sh
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_LOG_DIR=/var/log/hadoop
sudo mkdir /var/log/hadoop/
sudo chown -R hduser:hadoop /var/log/hadoop
 (do cd after above commands then do below steps)
((Below are the pre-requisites for hadoop deployment in production))

#Disable FireWall iptables
sudo iptables -L -n (this cmnd list out ip tables)
sudo ufw status (In ubuntu package name is ufw)
sudo ufw disable

#Disabling Transparent Hugepage Compaction

cat /sys/kernel/mm/transparent_hugepage/defrag (in this we can see kernal will diable it but we have to permanently disable hugpages)

$ sudo nano /etc/init.d/disable-transparent-hugepages

#!/bin/sh
### BEGIN INIT INFO
# Provides:          disable-transparent-hugepages
# Required-Start:    $local_fs
# Required-Stop:
# Default-Start:     2 3 4 5
# Default-Stop:      0 1 6
# Short-Description: Disable Linux transparent huge pages
# Description:       Disable Linux transparent huge pages, to improve
#                    database performance.
### END INIT INFO

case $1 in
  start)
    if [ -d /sys/kernel/mm/transparent_hugepage ]; then
      thp_path=/sys/kernel/mm/transparent_hugepage
    elif [ -d /sys/kernel/mm/redhat_transparent_hugepage ]; then
      thp_path=/sys/kernel/mm/redhat_transparent_hugepage
    else
      return 0
    fi

    echo 'never' > ${thp_path}/enabled
    echo 'never' > ${thp_path}/defrag

    unset thp_path
    ;;
esac 
(till esac copy and paste from #!)

$ sudo chmod 755 /etc/init.d/disable-transparent-hugepages

$ sudo update-rc.d disable-transparent-hugepages defaults ( meaning of this cmnd -it becames default entry -jab jab system restart hoga tab ye script chalegi or trnprnt hgpge ku disable kr dengi)

>>Restart server  (goto aws and reboot server then) (go into hduser from ubuntu after restart)


# Set Swappiness (it is done to diable swapiness for performance of namenode, bcos NN is served by RAM)
sudo sysctl -a | grep vm.swappiness
sudo sysctl vm.swappiness=1

# Configure NTP (we are distributed system so we need to have clock in synch hw clock,etc)
timedatectl status
timedatectl list-timezones
sudo timedatectl set-timezone Asia/Kolkata
sudo apt install ntp -y

##Configure SSH Password less logins (copy first two line)
sudo su -c touch /home/hduser/.ssh/config; echo "Host *\n StrictHostKeyChecking no\n
UserKnownHostsFile=/dev/null" > /home/hduser/.ssh/config 
(above both line copy and paste from sudo till config) 
(after that check in cd .ssh - doing ls then cat config)

sudo service ssh restart

# Configure .profile (make sure you are on NN) 
 nano .profile
 eval `ssh-agent` ssh-add /home/hduser/.ssh/abc.pem

 source .profile
(((After this create ami image from aws and make it enable, as we create ami all above the settings is configured on all instance which will be created under it)))

----------****** Create a snapshot at this point ******-----------------
Create 4 nodes from this image

#sudo nano /etc/hosts (this is the file were we can host the names and include these lines:FQDN)
172.31.25.94 ip-172-31-25-94.ec2.internal nn
172.31.27.5 ip-172-31-27-5.ec2.internal rm
172.31.27.233 ip-172-31-27-233.ec2.internal 1dn
172.31.27.40 ip-172-31-27-40.ec2.internal 2dn
172.31.27.44 ip-172-31-27-44.ec2.internal 3dn
Above Ips changes as per your connections to server please go through the recent of them.


(Below example given for ip change as per connection of server)
172.31.29.56 ip-172-31-29-56.ec2.internal nn
172.31.27.96 ip-172-31-27-96.ec2.internal rm
172.31.18.137 ip-172-31-18-137.ec2.internal 1dn
172.31.23.79 ip-172-31-23-79.ec2.internal 2dn
172.31.16.236 ip-172-31-16-236.ec2.internal 3dn

(Do this for all nodes by connecting through ssh rm,1dn,2dn,3dn)


# Install and Configure dsh
sudo apt install dsh -y
sudo nano /etc/dsh/machines.list
#localhost (give # comment on starting of file after nano and then write hosts name)
nn
rm
1dn
2dn
3dn

dsh -a uptime (this will shows the hosts run time and data)
dsh -a source .profile

cd /usr/local/hadoop/etc/hadoop

# Configure masters and slaves
nano masters (enter rm and ctrl+o ctrl+x)
rm

nano workers (enter #localhost and under that write all 1,2,3,nodes)
1dn
2dn
3dn

#Update core-site.xml 
nano core-site.xml (this is use for cluster wide operations)
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://nn:9000</value>
  </property>

#Update hdfs-site.xml on name node 
mkdir -p /usr/local/hadoop/data/hdfs/namenode (datanode ki dir only on datanode)
nano hdfs-site.xml (First do nano hdfs & paste property to property, then make above dir by mkdir)
<property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///usr/local/hadoop/data/hdfs/namenode</value>
  </property>
   <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///usr/local/hadoop/data/hdfs/datanode</value>
  </property> 


#Create proper directories on datanode's
dsh -m 1dn,2dn,3dn mkdir -p /usr/local/hadoop/data/hdfs/datanode (datanode ki dir datanode only)
 


#Update yarn-site.xml
nano yarn-site.xml
<property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>rm</value>
  </property>
<property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
  </property>


#Update mapred-site.xml

nano mapred-site.xml
<property>
    <name>mapreduce.jobtracker.address</name>
    <value>rm:54311</value>
  </property>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
<property>
  <name>yarn.app.mapreduce.am.env</name>
  <value>HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME</value>
</property>
<property>
  <name>mapreduce.map.env</name>
  <value>HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME</value>
</property>
<property>
  <name>mapreduce.reduce.env</name>
  <value>HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME</value>
</property>

sudo chown -R hduser:hadoop $HADOOP_HOME (all content which is under usr/hadoop should be save on hduser and we dnt have permision issue)

#SCP all the files (copy from one host to another host)
  
For all nodes
  scp core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml workers rm:/usr/local/hadoop/etc/hadoop
(send this profile config to all nodes by editing from "rm" to 1dn, 2dn, 3dn, in above given cmnd after this all the xml are available on all nodes)


#Format Namenode

hdfs namenode -format


# Start the cluster (You should start it on namenode bcos it is the master of hdfs)
start-dfs.sh
(after above cmnd do ssh rm and then below cmnd)
start-yarn.sh   >>>> RM is the master yarn<<<<
 
dsh -a jps >>> from this we can see all the daemans on all the host<<<<<
 (Till here we have successfully deploy cluster)
((Below are the data which is put for info in job))

hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/ubuntu
hdfs dfs -put hadoop-3.3.4o apt .tar.gz /user/ubuntu
hdfs dfs -ls 
hdfs dfs -ls -R

yarn jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar pi  5 10


WEB UI for Name node and Resource Manager:-
>>>> Goto aws instnc copy public ip of NN and paste in browser with :9870<<<<<<<<
>>>>>For RM goto aws inst copy public ip of rm & paste with :8088<<<<<<
